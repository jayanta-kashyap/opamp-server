## Scalable Device Configuration Management for 1M+ Devices

---

## Table of Contents

| Section | Description |
|---------|-------------|
| [1. Current POC Architecture](#1-current-poc-architecture) | Single-instance design in minikube |
| [2. Production Challenges](#2-production-challenges) | Why POC won't scale |
| [3. Proposed Production Architecture](#3-proposed-production-architecture) | High-level overview with Redis + Kafka |
| [4. Component Deep Dive](#4-component-deep-dive) | Server, Supervisor, Redis, Kafka details |
| [5. Data Flow Patterns](#5-data-flow-patterns) | Registration, commands, hot reload, DDS |
| [6. Technology Choices](#6-technology-choices) | Redis + Kafka recommendation |
| [7. Scaling Strategy](#7-scaling-strategy) | Capacity planning and sizing |
| [8. Implementation Roadmap](#8-implementation-roadmap) | Phased rollout plan |

**Quick Links:**
- [4.1 OpAMP Server](#41-opamp-server-stateless-api-layer) | [4.2 Supervisor Fleet](#42-supervisor-fleet-connection-managers) | [4.3 Redis](#43-rediselasticache-state-storage) | [4.4 Kafka](#44-kafka-message-bus)
- [5.1 Device Registration](#51-device-registration-flow) | [5.2 Command Flow](#52-command-flow-toggle-emission) | [5.3 Hot Reload](#53-hot-reload-flow-unchanged-from-poc) | [5.4 DDS Observability](#54-cloud-service-observability-serversupervisor--dds)

---

# Agenda

1. Current POC Architecture
2. Production Challenges
3. Proposed Production Architecture
4. Component Deep Dive
5. Data Flow Patterns
6. Technology Choices
7. Scaling Strategy
8. Implementation Roadmap

---

# 1. Current POC Architecture

## Single-Instance Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Minikube Cluster                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  opamp-control namespace                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   OpAMP    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   Server   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Supervisor  â”‚                 â”‚
â”‚  â”‚  (1 pod)   â”‚  WebSocket â”‚  (1 pod)    â”‚                 â”‚
â”‚  â”‚            â”‚            â”‚             â”‚                 â”‚
â”‚  â”‚ â€¢ REST API â”‚            â”‚ â€¢ gRPC      â”‚                 â”‚
â”‚  â”‚ â€¢ Dashboardâ”‚            â”‚ â€¢ In-memory â”‚                 â”‚
â”‚  â”‚ â€¢ In-memoryâ”‚            â”‚   device mapâ”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                   â”‚                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  opamp-edge namespace             â”‚ gRPC (50051)            â”‚
â”‚                                   â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Device      â”‚  â”‚ Device      â”‚  â”‚ Device      â”‚         â”‚
â”‚  â”‚ Agent 1     â”‚  â”‚ Agent 2     â”‚  â”‚ Agent N     â”‚         â”‚
â”‚  â”‚ + FluentBit â”‚  â”‚ + FluentBit â”‚  â”‚ + FluentBit â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## POC Characteristics

| Aspect | Current State |
|--------|---------------|
| **Devices** | 1 device |
| **Server Pods** | 1 |
| **Supervisor Pods** | 1 |
| **State Storage** | In-memory |
| **Message Passing** | Direct WebSocket |

âœ… **Works perfectly for POC and demo**

---

# 2. Production Challenges

## Why POC Architecture Won't Scale

### Problem 1: Single Point of Failure
```
Server Pod crashes â†’ All UI/API gone
Supervisor Pod crashes â†’ All 1M device connections lost
```

### Problem 2: Memory Limits
```
1M devices Ã— 1KB state each = 1GB+ memory per pod
Single pod can't hold this
```

### Problem 3: Connection Limits
```
Each supervisor pod can handle ~20,000 gRPC connections
1M devices Ã· 20K = 50 supervisor pods minimum
```

### Problem 4: Stateless Scaling
```
Pod 1 receives "toggle device-5"
But device-5 is connected to Pod 37
How does Pod 1 know this? â†’ Needs shared state
```

---

# 3. Proposed Production Architecture

## High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                Aruba Cloud (Kubernetes)                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  Control Plane Services                    â”‚   â”‚ DDS (CNX Common    â”‚  â”‚
â”‚  â”‚                                                            â”‚   â”‚ Observability)     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚                    â”‚  â”‚
â”‚  â”‚  â”‚  Redis/Elasticache â”‚    â”‚       Kafka        â”‚         â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚      (State)       â”‚    â”‚    (Messaging)     â”‚         â”‚   â”‚ â”‚ Grafana Loki   â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚ â”‚   (Logs)       â”‚ â”‚  â”‚
â”‚  â”‚            â”‚                         â”‚                    â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚            â–¼                         â–¼                    â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚ â”‚ Grafana Mimir  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚              OpAMP Servers (n pods)             â”‚â”€â”€â”€â”¼â”€â”€â”€â”¼â–ºâ”‚  (Metrics)     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚                                                    â”‚   â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ REST API    â€¢ Dashboard    â€¢ OTel SDK           â”‚   â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚ â”‚ Grafana Tempo  â”‚ â”‚  â”‚
â”‚  â”‚                           â”‚                               â”‚   â”‚ â”‚  (Traces)      â”‚ â”‚  â”‚
â”‚  â”‚                           â–¼                               â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚                    â”‚  â”‚
â”‚  â”‚  â”‚            Supervisor Fleet (m pods)             â”‚â”€â”€â”€â”¼â”€â”€â”€â”¼â–º   (OTLP export)   â”‚  â”‚
â”‚  â”‚  â”‚                                                    â”‚   â”‚   â”‚                    â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”    â€¢ OTel SDK        â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚  â”‚  â”‚S-1 â”‚ â”‚S-2 â”‚ â”‚S-3 â”‚ â”‚S-50â”‚   (20K devices each) â”‚   â”‚                           â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜                       â”‚   â”‚                           â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                           â”‚
â”‚  â”‚                           â”‚                               â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                              â”‚                                                           â”‚
â”‚                              â”‚ gRPC (bidirectional stream)                               â”‚
â”‚                              â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                          Edge / Campus / Devices                                   â”‚   â”‚
â”‚  â”‚                                                                                    â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚
â”‚  â”‚    â”‚ AP  â”‚   â”‚ AP  â”‚   â”‚ SW  â”‚   â”‚ GW  â”‚    ...       â”‚ 1M+ Devices â”‚             â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 4. Component Deep Dive

## 4.1 OpAMP Server (Stateless API Layer)

**Responsibilities:**
- REST API endpoints
- Web Dashboard
- Authenticate requests
- Route commands to correct supervisor via Kafka

**Scaling:**
- 3-10 pods behind load balancer
- Horizontally scalable
- No local state (reads from Redis)

```go
// Pseudo-code: Handle toggle command
func HandleToggle(deviceID string, state bool) {
    // 1. Lookup which supervisor owns this device
    supervisor := db.Query("SELECT supervisor_id FROM devices WHERE device_id = ?", deviceID)
    
    // 2. Publish command to Kafka
    kafka.Produce("opamp.commands", supervisor, Command{
        DeviceID: deviceID,
        Action:   "toggle",
        State:    state,
    })
}
```

---

## 4.2 Supervisor Fleet (Connection Managers)

**Responsibilities:**
- Maintain gRPC streams to devices
- Execute commands received from Kafka
- Report device status to Redis
- Handle config delivery and hot reload

**Scaling:**
- ~50 pods for 1M devices
- Each pod handles ~20K connections
- Stateful (holds connections in memory)

```go
// Pseudo-code: Supervisor startup
func StartSupervisor(supervisorID string) {
    // 1. Subscribe to commands for this supervisor
    kafka.Subscribe("opamp.commands", supervisorID)
    
    // 2. Accept device connections
    grpcServer.Serve(":50051")
    
    // 3. On device connect, register in Redis
    onDeviceConnect(device) {
        redis.Set("device:"+device.ID+":supervisor", supervisorID)
        redis.Expire("device:"+device.ID+":supervisor", 300) // 5min TTL
        redis.SAdd("supervisor:"+supervisorID+":devices", device.ID)
    }
}
```

---

## 4.3 Redis/Elasticache (State Storage)

**The Only Database OpAMP Needs**

Redis serves as the sole data store for OpAMP. No SQL database required.

### Why Redis Only?

| Requirement | Redis Solution | Why Not SQL? |
|-------------|----------------|---------------|
| Deviceâ†’Supervisor lookup | `GET device:X:supervisor` (0.5ms) | SQL is 10-20x slower |
| Device status | `HSET device:X status online` | Key-value is simpler |
| Config cache | `SET config:fluentbit:v1 <data>` | No schema needed |
| Auto-cleanup | `EXPIRE device:X:supervisor 300` | Built-in TTL |

### Complete Redis Data Model

```redis
# =====================================================
# DEVICE ROUTING (Critical Path - Every Command)
# =====================================================

# Which supervisor has this device? (THE most important key)
SET device:device-5:supervisor "supervisor-pod-3"
EXPIRE device:device-5:supervisor 300  # 5min TTL, auto-cleanup on disconnect

# =====================================================
# DEVICE STATUS (For Dashboard)
# =====================================================

# Device details as a hash
HSET device:device-5 \
    status "online" \
    emission "true" \
    config_version "v1.2.3" \
    agent_type "fluentbit" \
    last_seen "1706356800" \
    connected_at "1706350000"

# All online devices (for quick listing)
SADD devices:online "device-5" "device-6" "device-7"

# =====================================================
# SUPERVISOR TRACKING (For Load Balancing)
# =====================================================

# Which devices are on each supervisor?
SADD supervisor:supervisor-pod-3:devices "device-5" "device-100" "device-500"

# How many devices per supervisor? (for new connection routing)
INCR supervisor:supervisor-pod-3:device_count

# =====================================================
# CONFIG CACHE (Rarely Changes)
# =====================================================

# Config templates by type and version
SET config:fluentbit:v1.2.3 "[SERVICE]\n    hot_reload On\n..."
SET config:fluentbit:latest "v1.2.3"  # Pointer to current version
```

### Memory Calculation

```
1M devices:
â”œâ”€â”€ device:X:supervisor (1M keys Ã— 50 bytes) = 50 MB
â”œâ”€â”€ device:X hash (1M keys Ã— 100 bytes)     = 100 MB
â”œâ”€â”€ supervisor sets (50 sets Ã— 20K members)  = 10 MB
â””â”€â”€ config cache                             = 1 MB
                                              --------
                                     Total:   ~160 MB

Elasticache: Even smallest instance (cache.t3.micro = 0.5 GB) handles this easily.
Recommended: cache.r6g.large (13 GB) for headroom and replication.
```

---

## 4.4 Kafka (Message Bus)

**Topics:**

| Topic | Purpose | Key | Consumers |
|-------|---------|-----|-----------|
| `opamp.commands` | Server â†’ Supervisor commands | supervisor_id | Supervisor pods |
| `opamp.events` | Device â†’ Server events (optional) | device_id | Server pods |
| `opamp.config-updates` | Broadcast config changes | - | All supervisors |

**Why Kafka:**
- âœ… Already available in CNX
- âœ… Durable (commands not lost)
- âœ… Ordered delivery per partition
- âœ… Replay capability for recovery
- âœ… Scales to millions of messages/sec

---

# 5. Data Flow Patterns

## 5.1 Device Registration Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Device   â”‚     â”‚ Supervisor  â”‚     â”‚   Redis    â”‚
â”‚ (device-5) â”‚     â”‚   Pod 3     â”‚     â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                   â”‚                  â”‚
      â”‚ 1. gRPC Connect   â”‚                  â”‚
      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                  â”‚
      â”‚                   â”‚                  â”‚
      â”‚                   â”‚ 2. SET device:   â”‚
      â”‚                   â”‚    device-5:sup  â”‚
      â”‚                   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
      â”‚                   â”‚    (0.5ms)       â”‚
      â”‚                   â”‚                  â”‚
      â”‚ 3. Connection ACK â”‚                  â”‚
      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                  â”‚
      â”‚                   â”‚                  â”‚
```

---

## 5.2 Command Flow (Toggle Emission)

```
â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User â”‚    â”‚ Server â”‚    â”‚  Redis   â”‚    â”‚ Kafka â”‚    â”‚ Supervisor â”‚    â”‚ Device â”‚
â”‚      â”‚    â”‚ Pod 2  â”‚    â”‚          â”‚    â”‚       â”‚    â”‚   Pod 3    â”‚    â”‚device-5â”‚
â””â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚ 1. Toggle  â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚ device-5   â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚ 2. GET deviceâ”‚              â”‚              â”‚               â”‚
   â”‚            â”‚  :device-5:  â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚  supervisor  â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚              â”‚              â”‚               â”‚
   â”‚            â”‚   (0.5ms!)   â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚ 3. "pod-3"   â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚ 4. Produce command          â”‚              â”‚               â”‚
   â”‚            â”‚    to topic:pod-3           â”‚              â”‚               â”‚
   â”‚            â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚ 5. Consume   â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚ 6. gRPC       â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚ ConfigPush    â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚ 7. ConfigAck  â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚ 8. HSET      â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚  emission=on â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚ 9. Success â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚              â”‚              â”‚              â”‚               â”‚
```

**Key Point:** Step 2-3 (Redis lookup) takes **0.5ms** vs 5-10ms if we used SQL.

---

## 5.3 Hot Reload Flow (Unchanged from POC)

```
Device Agent                     FluentBit Container
     â”‚                                   â”‚
     â”‚ 1. Receive ConfigPush             â”‚
     â”‚    (new fluent-bit.conf)          â”‚
     â”‚                                   â”‚
     â”‚ 2. Write to /shared-config/       â”‚
     â”‚    fluent-bit.conf                â”‚
     â”‚                                   â”‚
     â”‚ 3. POST /api/v2/reload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚                                   â”‚
     â”‚                    4. Re-read config
     â”‚                    5. Apply new pipeline
     â”‚                                   â”‚
     â”‚ 6. {"status": 0} â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
     â”‚                                   â”‚
     â”‚ 7. Send ConfigAck                 â”‚
     â”‚    (success=true)                 â”‚
```

**No restart required** - FluentBit hot reload preserves:
- Log position
- Buffer state
- Active connections

---

## 5.4 Cloud Service Observability (Server/Supervisor â†’ DDS)

**Note:** This section describes observability for the OpAMP cloud services themselves (Server and Supervisor). This is separate from the edge device telemetry pipeline described in Section 5.3.

DDS is the **CNX observability platform**. The OpAMP cloud services (Server and Supervisor Go applications) are instrumented with OpenTelemetry SDK to send their telemetry to DDS, just like other CNX services.

### Key Design Principles

1. **No STDOUT/STDERR Logging** - Server and Supervisor code will NOT write logs to stdout/stderr. This ensures we do not accidentally send logs to Humio (which captures container stdout in CNX). All logging is done via OTel SDK exporters directly to DDS.

2. **OTel Exporters Only** - All three telemetry types (logs, metrics, traces) are sent exclusively via OTel OTLP exporters to DDS backends.

3. **Correlated Telemetry** - All three data types share trace context (trace_id, span_id), enabling seamless navigation in Grafana:
   - Jump from a log entry â†’ related trace
   - Jump from a trace span â†’ related logs
   - Jump from metrics â†’ exemplars â†’ traces
   - Full request lifecycle visibility across services

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         OpAMP Cloud Services                                â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚    OpAMP Server     â”‚         â”‚     Supervisor      â”‚                  â”‚
â”‚   â”‚     (3-10 pods)     â”‚         â”‚     (50+ pods)      â”‚                  â”‚
â”‚   â”‚                     â”‚         â”‚                     â”‚                  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚
â”‚   â”‚  â”‚ OTel SDK      â”‚  â”‚         â”‚  â”‚ OTel SDK      â”‚  â”‚                  â”‚
â”‚   â”‚  â”‚ - Traces      â”‚â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”‚ - Traces      â”‚  â”‚                  â”‚
â”‚   â”‚  â”‚ - Metrics     â”‚  â”‚ shared  â”‚  â”‚ - Metrics     â”‚  â”‚                  â”‚
â”‚   â”‚  â”‚ - Logs        â”‚  â”‚ context â”‚  â”‚ - Logs        â”‚  â”‚                  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚
â”‚   â”‚          â”‚          â”‚         â”‚          â”‚          â”‚                  â”‚
â”‚   â”‚  âŒ No STDOUT       â”‚         â”‚  âŒ No STDOUT       â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚              â”‚                               â”‚                              â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼  OTLP (gRPC/HTTP)                            â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚              â”‚      OTel Collector           â”‚  (optional, or direct)       â”‚
â”‚              â”‚      (sidecar or central)     â”‚                              â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                              â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DDS (CNX Common Observability Platform)                     â”‚
â”‚                                                                              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â”‚  Grafana Loki  â”‚â—„â”€â”€â”€â”‚â”€â”€â”€â”€â”€ trace_id â”€â”€â”€â”€â”€â–ºâ”‚ Grafana Tempo  â”‚           â”‚
â”‚    â”‚    (Logs)      â”‚    â”‚                â”‚    â”‚   (Traces)     â”‚           â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Correlated   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚            â”‚             â”‚   via OTel     â”‚            â”‚                    â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   Context      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                          â”‚                â”‚                                  â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚                                  â”‚
â”‚                    â”‚Grafana Mimir â”‚â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                    â”‚  (Metrics)   â”‚  exemplars â†’ traces                      â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                                              â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                        â”‚      Grafana       â”‚                                â”‚
â”‚                        â”‚   (Dashboards)     â”‚                                â”‚
â”‚                        â”‚  Logs â†” Traces â†”   â”‚                                â”‚
â”‚                        â”‚      Metrics       â”‚                                â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Gets Instrumented

| Service | Telemetry Type | What It Captures |
|---------|----------------|------------------|
| **OpAMP Server** | Traces | API request latency, Redis/Kafka calls |
| **OpAMP Server** | Metrics | Request count, error rate, queue depth |
| **OpAMP Server** | Logs | Structured logs (JSON) |
| **Supervisor** | Traces | Device connection lifecycle, config push latency |
| **Supervisor** | Metrics | Connected devices, commands processed, errors |
| **Supervisor** | Logs | Device events, config changes |

### Go OTel Instrumentation Example

```go
import (
    "context"
    
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
    "go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploggrpc"
    "go.opentelemetry.io/otel/log"
    "go.opentelemetry.io/otel/sdk/trace"
    sdklog "go.opentelemetry.io/otel/sdk/log"
)

var logger log.Logger

func initTelemetry(ctx context.Context) {
    // Trace exporter â†’ Tempo
    traceExp, _ := otlptracegrpc.New(ctx,
        otlptracegrpc.WithEndpoint("tempo.dds.aruba.cloud:4317"),
    )
    tp := trace.NewTracerProvider(trace.WithBatcher(traceExp))
    otel.SetTracerProvider(tp)
    
    // Log exporter â†’ Loki (via OTel Collector)
    // NO STDOUT - logs go directly to DDS
    logExp, _ := otlploggrpc.New(ctx,
        otlploggrpc.WithEndpoint("loki.dds.aruba.cloud:4317"),
    )
    lp := sdklog.NewLoggerProvider(sdklog.WithProcessor(
        sdklog.NewBatchProcessor(logExp),
    ))
    logger = lp.Logger("opamp-server")
}

// Usage in handler - trace context automatically correlates logs
func HandleToggle(ctx context.Context, deviceID string) {
    ctx, span := otel.Tracer("opamp-server").Start(ctx, "HandleToggle")
    defer span.End()
    
    span.SetAttributes(attribute.String("device.id", deviceID))
    
    // âŒ NEVER: fmt.Println() or log.Printf() - goes to stdout/Humio
    // âœ… ALWAYS: OTel logger - goes to DDS with trace correlation
    logger.Emit(ctx, log.Record{
        Severity: log.SeverityInfo,
        Body:     log.StringValue("Processing toggle command"),
        Attributes: []log.KeyValue{
            log.String("device.id", deviceID),
        },
    })
    
    // ... business logic
}
```

### Key Metrics to Expose

| Metric | Type | Labels |
|--------|------|--------|
| `opamp_commands_total` | Counter | action, status |
| `opamp_config_push_duration_seconds` | Histogram | device_type |
| `opamp_connected_devices` | Gauge | supervisor_pod |
| `opamp_redis_latency_seconds` | Histogram | operation |
| `opamp_kafka_messages_total` | Counter | topic, status |

---

# 6. Technology Choices

## Recommendation: Redis + Kafka

After analyzing the available infrastructure in Aruba, we recommend using **only Redis and Kafka** for OpAMP.

### What OpAMP Actually Needs

| Operation | Frequency | Latency Need | Data Pattern |
|-----------|-----------|--------------|---------------|
| **"Which supervisor has device-X?"** | Every command | < 5ms | Key-value lookup |
| Device status updates | Every heartbeat | < 10ms | Key-value write |
| Config cache | On change | Not critical | Key-value |
| Command delivery | Every action | < 100ms | Pub/sub |

### Why Redis?

```
OpAMP Core Question: "Which supervisor has device-5?"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Redis      â”‚  GET device:device-5:supervisor
â”‚    0.5 ms âœ…    â”‚  â†’ "supervisor-pod-3"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… Sub-millisecond lookups (critical for 1M+ devices)
âœ… Built-in TTL (auto-cleanup when devices disconnect)
âœ… Simple key-value model (exactly what we need)
âœ… Already available in Aruba (Elasticache)
âœ… Trivial memory footprint (~200MB for 1M devices)
```

### Redis Data Model (Complete)

```redis
# Core routing - THE critical path (every command uses this)
SET device:device-5:supervisor "supervisor-pod-3"
EXPIRE device:device-5:supervisor 300  # Auto-cleanup on disconnect

# Device status (for dashboard)
HSET device:device-5 status "online" emission "true" config_version "v1.2.3"

# Supervisor tracking (for load balancing new connections)
INCR supervisor:supervisor-pod-3:device_count
SADD supervisor:supervisor-pod-3:devices "device-5"

# Config templates (cached, rarely changes)
SET config:fluentbit:v1.2.3 "<config data>"
```

**Total Redis memory for 1M devices: ~100-200 MB** (trivial)

### Why Kafka (Not Direct Calls)

| Without Kafka | With Kafka |
|---------------|------------|
| Server must know all supervisor IPs | Server publishes to topic |
| If supervisor down, command lost | Command persisted, delivered when supervisor recovers |
| Tight coupling | Loose coupling |
| No audit trail | Kafka retention = command history |

### Summary: What OpAMP Uses

| Component | Purpose |
|-----------|--------|
| **Redis/Elasticache** | Deviceâ†’Supervisor routing, status cache, config templates |
| **Kafka** | Command delivery, durability, audit trail |
| **DDS** | Observability platform for cloud services (Server, Supervisor) telemetry |

### DDS Integration (Cloud Services Observability)

| DDS Component | Signal | Source | Use Case |
|---------------|--------|--------|----------|
| **Grafana Loki** | Logs | Server + Supervisor pods | Structured logs, error tracking |
| **Grafana Mimir** | Metrics | Server + Supervisor pods | Request rates, latency, device counts |
| **Grafana Tempo** | Traces | Server + Supervisor pods | Request tracing, end-to-end latency |

> **Note:** Other Aruba databases (CockroachDB, ArangoDB, ClickHouse) are not needed for OpAMP core functionality. They can be added later if analytics or audit log queries are required.

## Existing Infrastructure Leverage

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Used for OpAMP (Already Have)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… Redis/Elasticache (state + routing) â”‚
â”‚  âœ… Kafka (command delivery)            â”‚
â”‚  âœ… DDS (Loki, Mimir, Tempo)            â”‚
â”‚  âœ… Kubernetes                          â”‚
â”‚  âœ… Load balancers                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Available but NOT Needed for OpAMP    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â¸ï¸  CockroachDB (keep for other uses)   â”‚
â”‚  â¸ï¸  ArangoDB (keep for other uses)      â”‚
â”‚  â¸ï¸  ClickHouse (keep for other uses)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          New Components                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ†• OpAMP Server pods (OTel instrumented)|
â”‚  ğŸ†• Supervisor pods (OTel instrumented) â”‚
â”‚  ğŸ†• Device agents (on each device)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## The Simple Truth

```
OpAMP needs to answer ONE question fast:
"Which supervisor has device-X?"

Redis: 0.5ms
Everything else: Slower or wrong tool.

OpAMP needs to deliver commands reliably:
Kafka: Durable, ordered, already available.

That's it. Keep it simple.
```

---

# 7. Scaling Strategy

## Capacity Planning

| Scale | Devices | Supervisors | Servers | Kafka Partitions |
|-------|---------|-------------|---------|------------------|
| Small | 10K | 1 | 2 | 4 |
| Medium | 100K | 5 | 3 | 16 |
| Large | 500K | 25 | 5 | 32 |
| Enterprise | 1M+ | 50+ | 10 | 64 |

## Supervisor Pod Sizing

```
Each Supervisor Pod:
â”œâ”€â”€ Memory: 2-4 GB
â”‚   â”œâ”€â”€ 20K connections Ã— 50KB each = 1GB
â”‚   â””â”€â”€ Overhead, buffers = 1-3GB
â”œâ”€â”€ CPU: 2-4 cores
â”‚   â””â”€â”€ gRPC handling, config processing
â””â”€â”€ Network: 1 Gbps
    â””â”€â”€ Config pushes, heartbeats
```

## Redis Sizing

```
For 1M devices:
â”œâ”€â”€ Keys: ~3M (device routing + status + configs)
â”œâ”€â”€ Average value size: ~50 bytes
â”œâ”€â”€ Total memory: ~150-200 MB
â”œâ”€â”€ Elasticache node: cache.r6g.large (plenty of headroom)
â””â”€â”€ Replication: 1 primary + 1 replica for HA
```

---

# 8. Implementation Roadmap

## Phase 1: POC Enhancement (Current)
- [x] Single server, single supervisor
- [x] 1 device working
- [x] Hot reload with FluentBit API
- [x] Dashboard with toggle controls

## Phase 2: Add Shared State (2-3 weeks)
- [x] Redis/Elasticache *(already available in CNX cluster)*
- [x] Kafka *(already available in CNX cluster)*
- [ ] Migrate device registry to Redis
- [ ] Add Kafka producer to Server
- [ ] Add Kafka consumer to Supervisor
- [ ] Add OTel instrumentation (no stdout, export to DDS)

## Phase 3: Multi-Pod Deployment (2-3 weeks)
- [ ] Scale Server to 3 replicas
- [ ] Scale Supervisor to 5 replicas
- [ ] Test failover scenarios
- [ ] Load test with 1000 devices

## Phase 4: Production Hardening (4-6 weeks)
- [ ] Add authentication/authorization
- [ ] Implement rate limiting
- [ ] Add comprehensive monitoring (DDS dashboards)
- [ ] Runbook and documentation
- [ ] Security audit

## Phase 5: Scale to 1M (Ongoing)
- [ ] Gradual rollout to production devices
- [ ] Performance tuning
- [ ] Capacity expansion as needed

---

# Summary

## Architecture Benefits

| Benefit | How Achieved |
|---------|--------------|
| **High Availability** | Multiple Server & Supervisor pods |
| **Horizontal Scale** | Stateless servers, partitioned supervisors |
| **Durability** | Redis for state, Kafka for commands |
| **Low Latency** | gRPC streaming, Redis lookups (0.5ms) |
| **Auditability** | All commands logged to Kafka (retention) |
| **Observability** | Cloud services (Server/Supervisor) instrumented with OTel â†’ DDS |
| **Operational Simplicity** | Uses existing Redis + Kafka + DDS infrastructure |

## Key Metrics to Monitor

| Metric | Target |
|--------|--------|
| Config push latency | < 1 second |
| Hot reload success rate | > 99.9% |
| Device connection uptime | > 99.95% |
| Command delivery latency | < 500ms |

---

# Questions?

## Contact

- **POC Repository**: opamp-server, opamp-supervisor, opamp-device-agent
- **Minikube Profile**: control-plane
- **Namespaces**: opamp-control, opamp-edge

---

# Appendix A: Protocol Details

## OpAMP (Server â†” Supervisor)

```
Protocol: WebSocket
Port: 4320
Library: open-telemetry/opamp-go
Direction: Bidirectional
Messages: AgentToServer, ServerToAgent
```

## Custom gRPC (Supervisor â†” Device)

```protobuf
service ControlService {
  rpc Control(stream Envelope) returns (stream Envelope);
}

message Envelope {
  oneof body {
    EdgeIdentity register = 1;
    Command command = 2;
    Event event = 3;
    ConfigPush config_push = 4;
    ConfigAck config_ack = 5;
  }
}
```

---

# Appendix B: Kafka Topic Configuration

```yaml
# opamp.commands topic
Topic: opamp.commands
Partitions: 64 (one per supervisor or hash-based)
Replication: 3
Retention: 7 days
Key: supervisor_id
Value: JSON command payload

# Message format
{
  "device_id": "device-5",
  "command": "toggle_emission",
  "payload": {
    "state": true,
    "config_version": "v1.2.3"
  },
  "timestamp": "2026-01-27T10:30:00Z",
  "correlation_id": "uuid-here"
}
```

---

# Appendix C: Database Schema (Complete)

```sql
-- Core Tables
CREATE TABLE device_registry (
    device_id         VARCHAR(64) PRIMARY KEY,
    supervisor_id     VARCHAR(64) NOT NULL,
    agent_type        VARCHAR(32) DEFAULT 'fluentbit',
    platform          VARCHAR(32),
    version           VARCHAR(32),
    connected_at      TIMESTAMP DEFAULT NOW(),
    last_seen         TIMESTAMP DEFAULT NOW(),
    config_version    VARCHAR(64),
    effective_config  TEXT,
    emission_state    BOOLEAN DEFAULT false,
    
    INDEX idx_supervisor (supervisor_id),
    INDEX idx_agent_type (agent_type),
    INDEX idx_last_seen (last_seen)
);

CREATE TABLE config_templates (
    template_id       VARCHAR(64) PRIMARY KEY,
    name              VARCHAR(128),
    agent_type        VARCHAR(32),
    version           VARCHAR(32),
    config_data       TEXT NOT NULL,
    is_active         BOOLEAN DEFAULT true,
    created_by        VARCHAR(64),
    created_at        TIMESTAMP DEFAULT NOW(),
    updated_at        TIMESTAMP DEFAULT NOW()
);

CREATE TABLE command_audit (
    id                BIGSERIAL PRIMARY KEY,
    device_id         VARCHAR(64),
    supervisor_id     VARCHAR(64),
    command_type      VARCHAR(32),
    payload           JSONB,
    correlation_id    VARCHAR(64),
    status            VARCHAR(16),
    error_message     TEXT,
    initiated_by      VARCHAR(64),
    created_at        TIMESTAMP DEFAULT NOW(),
    completed_at      TIMESTAMP,
    
    INDEX idx_device_id (device_id),
    INDEX idx_created_at (created_at)
);

CREATE TABLE supervisor_registry (
    supervisor_id     VARCHAR(64) PRIMARY KEY,
    pod_name          VARCHAR(128),
    pod_ip            VARCHAR(45),
    device_count      INTEGER DEFAULT 0,
    capacity          INTEGER DEFAULT 20000,
    last_heartbeat    TIMESTAMP DEFAULT NOW(),
    status            VARCHAR(16) DEFAULT 'active',
    
    INDEX idx_status (status)
);
```
