## Scalable Device Configuration Management for 1M+ Devices

---

## Table of Contents

| Section | Description |
|---------|-------------|
| [1. Production Architecture](#1-production-architecture) | High-level overview with Redis + Kafka |
| [2. Component Deep Dive](#2-component-deep-dive) | Server, Supervisor, Redis, Kafka details |
| [3. Data Flow Patterns](#3-data-flow-patterns) | Registration, commands, hot reload, DDS |
| [4. Technology Choices](#4-technology-choices) | Redis + Kafka recommendation |
| [5. Scaling Strategy](#5-scaling-strategy) | Capacity planning and sizing |
| [6. UI at Scale](#6-ui-at-scale) | Dashboard design for 1M devices |
| [7. Implementation Roadmap](#7-implementation-roadmap) | Phased rollout plan |

**Quick Links:**
- [2.1 OpAMP Server](#21-opamp-server-stateless-api-layer) | [2.2 Supervisor Fleet](#22-supervisor-fleet-connection-managers) | [2.3 Redis](#23-rediselasticache-state-storage) | [2.4 Kafka](#24-kafka-message-bus)
- [3.1 Device Registration](#31-device-registration-flow) | [3.2 Command Flow](#32-command-flow-toggle-emission) | [3.3 Hot Reload](#33-hot-reload-flow-unchanged-from-poc) | [3.4 DDS Observability](#34-cloud-service-observability-serversupervisor--dds)

---

# Agenda

1. Production Architecture
2. Component Deep Dive
3. Data Flow Patterns
4. Technology Choices
5. Scaling Strategy
6. UI at Scale
7. Implementation Roadmap

---

# 1. Production Architecture

## High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                Aruba Cloud (Kubernetes)                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  Control Plane Services                    â”‚   â”‚ DDS (CNX Common    â”‚  â”‚
â”‚  â”‚                                                            â”‚   â”‚ Observability)     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚                    â”‚  â”‚
â”‚  â”‚  â”‚  Redis/Elasticache â”‚    â”‚       Kafka        â”‚         â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚      (State)       â”‚    â”‚    (Messaging)     â”‚         â”‚   â”‚ â”‚ Grafana Loki   â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚ â”‚   (Logs)       â”‚ â”‚  â”‚
â”‚  â”‚            â”‚                         â”‚                    â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚            â–¼                         â–¼                    â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚ â”‚ Grafana Mimir  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚              OpAMP Servers (n pods)             â”‚â”€â”€â”€â”¼â”€â”€â”€â”¼â–ºâ”‚  (Metrics)     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚                                                    â”‚   â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ REST API    â€¢ Dashboard    â€¢ OTel SDK           â”‚   â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚ â”‚ Grafana Tempo  â”‚ â”‚  â”‚
â”‚  â”‚                           â”‚                               â”‚   â”‚ â”‚  (Traces)      â”‚ â”‚  â”‚
â”‚  â”‚                           â–¼                               â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚                    â”‚  â”‚
â”‚  â”‚  â”‚            Supervisor Fleet (m pods)             â”‚â”€â”€â”€â”¼â”€â”€â”€â”¼â–º   (OTLP export)   â”‚  â”‚
â”‚  â”‚  â”‚                                                    â”‚   â”‚   â”‚                    â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”    â€¢ OTel SDK        â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚  â”‚  â”‚S-1 â”‚ â”‚S-2 â”‚ â”‚S-3 â”‚ â”‚S-50â”‚   (20K devices each) â”‚   â”‚                           â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜                       â”‚   â”‚                           â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                           â”‚
â”‚  â”‚                           â”‚                               â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                              â”‚                                                           â”‚
â”‚                              â”‚ gRPC (bidirectional stream)                               â”‚
â”‚                              â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                          Edge / Campus / Devices                                   â”‚   â”‚
â”‚  â”‚                                                                                    â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚
â”‚  â”‚    â”‚ AP  â”‚   â”‚ AP  â”‚   â”‚ SW  â”‚   â”‚ GW  â”‚    ...       â”‚ 1M+ Devices â”‚             â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 2. Component Deep Dive

## 2.1 OpAMP Server (Stateless API Layer)

**Responsibilities:**
- REST API endpoints
- Web Dashboard
- Authenticate requests
- Route commands to correct supervisor via Kafka

**Scaling:**
- 3-10 pods behind load balancer
- Horizontally scalable
- No local state (reads from Redis)

```go
// Pseudo-code: Handle toggle command
func HandleToggle(deviceID string, state bool) {
    // 1. Lookup which supervisor owns this device
    supervisor := db.Query("SELECT supervisor_id FROM devices WHERE device_id = ?", deviceID)
    
    // 2. Publish command to Kafka
    kafka.Produce("opamp.commands", supervisor, Command{
        DeviceID: deviceID,
        Action:   "toggle",
        State:    state,
    })
}
```

---

## 2.2 Supervisor Fleet (Connection Managers)

**Responsibilities:**
- Maintain gRPC streams to devices
- Execute commands received from Kafka
- Report device status to Redis
- Handle config delivery and hot reload

**Scaling:**
- ~50 pods for 1M devices
- Each pod handles ~20K connections
- Stateful (holds connections in memory)

```go
// Pseudo-code: Supervisor startup
func StartSupervisor(supervisorID string) {
    // 1. Subscribe to commands for this supervisor
    kafka.Subscribe("opamp.commands", supervisorID)
    
    // 2. Accept device connections
    grpcServer.Serve(":50051")
    
    // 3. On device connect, register in Redis
    onDeviceConnect(device) {
        redis.Set("device:"+device.ID+":supervisor", supervisorID)
        redis.Expire("device:"+device.ID+":supervisor", 300) // 5min TTL
        redis.SAdd("supervisor:"+supervisorID+":devices", device.ID)
    }
}
```

---

## 2.3 Redis/Elasticache (State Storage)

**The Only Database OpAMP Needs**

Redis serves as the sole data store for OpAMP. No SQL database required.

### Why Redis Only?

| Requirement | Redis Solution | Why Not SQL? |
|-------------|----------------|---------------|
| Deviceâ†’Supervisor lookup | `GET device:X:supervisor` (0.5ms) | SQL is 10-20x slower |
| Device status | `HSET device:X status online` | Key-value is simpler |
| Config cache | `SET config:fluentbit:v1 <data>` | No schema needed |
| Auto-cleanup | `EXPIRE device:X:supervisor 300` | Built-in TTL |

### Complete Redis Data Model

```redis
# =====================================================
# DEVICE ROUTING (Critical Path - Every Command)
# =====================================================

# Which supervisor has this device? (THE most important key)
SET device:device-5:supervisor "supervisor-pod-3"
EXPIRE device:device-5:supervisor 300  # 5min TTL, auto-cleanup on disconnect

# =====================================================
# DEVICE STATUS (For Dashboard)
# =====================================================

# Device details as a hash
HSET device:device-5 \
    status "online" \
    emission "true" \
    config_version "v1.2.3" \
    agent_type "fluentbit" \
    last_seen "1706356800" \
    connected_at "1706350000"

# All online devices (for quick listing)
SADD devices:online "device-5" "device-6" "device-7"

# =====================================================
# SUPERVISOR TRACKING (For Load Balancing)
# =====================================================

# Which devices are on each supervisor?
SADD supervisor:supervisor-pod-3:devices "device-5" "device-100" "device-500"

# How many devices per supervisor? (for new connection routing)
INCR supervisor:supervisor-pod-3:device_count

# =====================================================
# CONFIG CACHE (Rarely Changes)
# =====================================================

# Config templates by type and version
SET config:fluentbit:v1.2.3 "[SERVICE]\n    hot_reload On\n..."
SET config:fluentbit:latest "v1.2.3"  # Pointer to current version
```

### Memory Calculation

```
1M devices:
â”œâ”€â”€ device:X:supervisor (1M keys Ã— 50 bytes) = 50 MB
â”œâ”€â”€ device:X hash (1M keys Ã— 100 bytes)     = 100 MB
â”œâ”€â”€ supervisor sets (50 sets Ã— 20K members)  = 10 MB
â””â”€â”€ config cache                             = 1 MB
                                              --------
                                     Total:   ~160 MB

Elasticache: Even smallest instance (cache.t3.micro = 0.5 GB) handles this easily.
Recommended: cache.r6g.large (13 GB) for headroom and replication.
```

---

## 2.4 Kafka (Message Bus)

**Topics:**

| Topic | Purpose | Key | Consumers |
|-------|---------|-----|-----------|
| `opamp.commands` | Server â†’ Supervisor commands | supervisor_id | Supervisor pods |
| `opamp.events` | Device â†’ Server events (optional) | device_id | Server pods |
| `opamp.config-updates` | Broadcast config changes | - | All supervisors |

**Why Kafka:**
- âœ… Already available in CNX
- âœ… Durable (commands not lost)
- âœ… Ordered delivery per partition
- âœ… Replay capability for recovery
- âœ… Scales to millions of messages/sec

---

# 3. Data Flow Patterns

## 3.1 Device Registration Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Device   â”‚     â”‚ Supervisor  â”‚     â”‚   Redis    â”‚
â”‚ (device-5) â”‚     â”‚   Pod 3     â”‚     â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                   â”‚                  â”‚
      â”‚ 1. gRPC Connect   â”‚                  â”‚
      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                  â”‚
      â”‚                   â”‚                  â”‚
      â”‚                   â”‚ 2. SET device:   â”‚
      â”‚                   â”‚    device-5:sup  â”‚
      â”‚                   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
      â”‚                   â”‚    (0.5ms)       â”‚
      â”‚                   â”‚                  â”‚
      â”‚ 3. Connection ACK â”‚                  â”‚
      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                  â”‚
      â”‚                   â”‚                  â”‚
```

---

## 3.2 Command Flow (Toggle Emission)

```
â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User â”‚    â”‚ Server â”‚    â”‚  Redis   â”‚    â”‚ Kafka â”‚    â”‚ Supervisor â”‚    â”‚ Device â”‚
â”‚      â”‚    â”‚ Pod 2  â”‚    â”‚          â”‚    â”‚       â”‚    â”‚   Pod 3    â”‚    â”‚device-5â”‚
â””â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚ 1. Toggle  â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚ device-5   â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚ 2. GET deviceâ”‚              â”‚              â”‚               â”‚
   â”‚            â”‚  :device-5:  â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚  supervisor  â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚              â”‚              â”‚               â”‚
   â”‚            â”‚   (0.5ms!)   â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚ 3. "pod-3"   â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚ 4. Produce command          â”‚              â”‚               â”‚
   â”‚            â”‚    to topic:pod-3           â”‚              â”‚               â”‚
   â”‚            â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚ 5. Consume   â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚ 6. gRPC       â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚ ConfigPush    â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚ 7. ConfigAck  â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚ 8. HSET      â”‚              â”‚               â”‚
   â”‚            â”‚              â”‚  emission=on â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚               â”‚
   â”‚            â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚ 9. Success â”‚              â”‚              â”‚              â”‚               â”‚
   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚              â”‚              â”‚              â”‚               â”‚
```

**Key Point:** Step 2-3 (Redis lookup) takes **0.5ms** vs 5-10ms if we used SQL.

---

## 3.3 Hot Reload Flow (Unchanged from POC)

```
Device Agent                     FluentBit Container
     â”‚                                   â”‚
     â”‚ 1. Receive ConfigPush             â”‚
     â”‚    (new fluent-bit.conf)          â”‚
     â”‚                                   â”‚
     â”‚ 2. Write to /shared-config/       â”‚
     â”‚    fluent-bit.conf                â”‚
     â”‚                                   â”‚
     â”‚ 3. POST /api/v2/reload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚                                   â”‚
     â”‚                    4. Re-read config
     â”‚                    5. Apply new pipeline
     â”‚                                   â”‚
     â”‚ 6. {"status": 0} â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
     â”‚                                   â”‚
     â”‚ 7. Send ConfigAck                 â”‚
     â”‚    (success=true)                 â”‚
```

**No restart required** - FluentBit hot reload preserves:
- Log position
- Buffer state
- Active connections

---

## 3.4 Cloud Service Observability (Server/Supervisor â†’ DDS)

**Note:** This section describes observability for the OpAMP cloud services themselves (Server and Supervisor). This is separate from the edge device telemetry pipeline described in Section 5.3.

DDS is the **CNX observability platform**. The OpAMP cloud services (Server and Supervisor Go applications) are instrumented with OpenTelemetry SDK to send their telemetry to DDS, just like other CNX services.

### Key Design Principles

1. **No STDOUT/STDERR Logging** - Server and Supervisor code will NOT write logs to stdout/stderr. This ensures we do not accidentally send logs to Humio (which captures container stdout in CNX). All logging is done via OTel SDK exporters directly to DDS.

2. **OTel Exporters Only** - All three telemetry types (logs, metrics, traces) are sent exclusively via OTel OTLP exporters to DDS backends.

3. **Correlated Telemetry** - All three data types share trace context (trace_id, span_id), enabling seamless navigation in Grafana:
   - Jump from a log entry â†’ related trace
   - Jump from a trace span â†’ related logs
   - Jump from metrics â†’ exemplars â†’ traces
   - Full request lifecycle visibility across services

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         OpAMP Cloud Services                                â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚    OpAMP Server     â”‚         â”‚     Supervisor      â”‚                  â”‚
â”‚   â”‚     (3-10 pods)     â”‚         â”‚     (50+ pods)      â”‚                  â”‚
â”‚   â”‚                     â”‚         â”‚                     â”‚                  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚
â”‚   â”‚  â”‚ OTel SDK      â”‚  â”‚         â”‚  â”‚ OTel SDK      â”‚  â”‚                  â”‚
â”‚   â”‚  â”‚ - Traces      â”‚â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”‚ - Traces      â”‚  â”‚                  â”‚
â”‚   â”‚  â”‚ - Metrics     â”‚  â”‚ shared  â”‚  â”‚ - Metrics     â”‚  â”‚                  â”‚
â”‚   â”‚  â”‚ - Logs        â”‚  â”‚ context â”‚  â”‚ - Logs        â”‚  â”‚                  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚
â”‚   â”‚          â”‚          â”‚         â”‚          â”‚          â”‚                  â”‚
â”‚   â”‚  âŒ No STDOUT       â”‚         â”‚  âŒ No STDOUT       â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚              â”‚                               â”‚                              â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼  OTLP (gRPC/HTTP)                            â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚              â”‚      OTel Collector           â”‚  (optional, or direct)       â”‚
â”‚              â”‚      (sidecar or central)     â”‚                              â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                              â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DDS (CNX Common Observability Platform)                     â”‚
â”‚                                                                              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â”‚  Grafana Loki  â”‚â—„â”€â”€â”€â”‚â”€â”€â”€â”€â”€ trace_id â”€â”€â”€â”€â”€â–ºâ”‚ Grafana Tempo  â”‚           â”‚
â”‚    â”‚    (Logs)      â”‚    â”‚                â”‚    â”‚   (Traces)     â”‚           â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Correlated   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚            â”‚             â”‚   via OTel     â”‚            â”‚                    â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   Context      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                          â”‚                â”‚                                  â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚                                  â”‚
â”‚                    â”‚Grafana Mimir â”‚â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                    â”‚  (Metrics)   â”‚  exemplars â†’ traces                      â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                                              â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                        â”‚      Grafana       â”‚                                â”‚
â”‚                        â”‚   (Dashboards)     â”‚                                â”‚
â”‚                        â”‚  Logs â†” Traces â†”   â”‚                                â”‚
â”‚                        â”‚      Metrics       â”‚                                â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Gets Instrumented

| Service | Telemetry Type | What It Captures |
|---------|----------------|------------------|
| **OpAMP Server** | Traces | API request latency, Redis/Kafka calls |
| **OpAMP Server** | Metrics | Request count, error rate, queue depth |
| **OpAMP Server** | Logs | Structured logs (JSON) |
| **Supervisor** | Traces | Device connection lifecycle, config push latency |
| **Supervisor** | Metrics | Connected devices, commands processed, errors |
| **Supervisor** | Logs | Device events, config changes |

### Go OTel Instrumentation Example

```go
import (
    "context"
    
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
    "go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploggrpc"
    "go.opentelemetry.io/otel/log"
    "go.opentelemetry.io/otel/sdk/trace"
    sdklog "go.opentelemetry.io/otel/sdk/log"
)

var logger log.Logger

func initTelemetry(ctx context.Context) {
    // Trace exporter â†’ Tempo
    traceExp, _ := otlptracegrpc.New(ctx,
        otlptracegrpc.WithEndpoint("tempo.dds.aruba.cloud:4317"),
    )
    tp := trace.NewTracerProvider(trace.WithBatcher(traceExp))
    otel.SetTracerProvider(tp)
    
    // Log exporter â†’ Loki (via OTel Collector)
    // NO STDOUT - logs go directly to DDS
    logExp, _ := otlploggrpc.New(ctx,
        otlploggrpc.WithEndpoint("loki.dds.aruba.cloud:4317"),
    )
    lp := sdklog.NewLoggerProvider(sdklog.WithProcessor(
        sdklog.NewBatchProcessor(logExp),
    ))
    logger = lp.Logger("opamp-server")
}

// Usage in handler - trace context automatically correlates logs
func HandleToggle(ctx context.Context, deviceID string) {
    ctx, span := otel.Tracer("opamp-server").Start(ctx, "HandleToggle")
    defer span.End()
    
    span.SetAttributes(attribute.String("device.id", deviceID))
    
    // âŒ NEVER: fmt.Println() or log.Printf() - goes to stdout/Humio
    // âœ… ALWAYS: OTel logger - goes to DDS with trace correlation
    logger.Emit(ctx, log.Record{
        Severity: log.SeverityInfo,
        Body:     log.StringValue("Processing toggle command"),
        Attributes: []log.KeyValue{
            log.String("device.id", deviceID),
        },
    })
    
    // ... business logic
}
```

### Key Metrics to Expose

| Metric | Type | Labels |
|--------|------|--------|
| `opamp_commands_total` | Counter | action, status |
| `opamp_config_push_duration_seconds` | Histogram | device_type |
| `opamp_connected_devices` | Gauge | supervisor_pod |
| `opamp_redis_latency_seconds` | Histogram | operation |
| `opamp_kafka_messages_total` | Counter | topic, status |

---

# 4. Technology Choices

## Recommendation: Redis + Kafka

After analyzing the available infrastructure in Aruba, we recommend using **only Redis and Kafka** for OpAMP.

### What OpAMP Actually Needs

| Operation | Frequency | Latency Need | Data Pattern |
|-----------|-----------|--------------|---------------|
| **"Which supervisor has device-X?"** | Every command | < 5ms | Key-value lookup |
| Device status updates | Every heartbeat | < 10ms | Key-value write |
| Config cache | On change | Not critical | Key-value |
| Command delivery | Every action | < 100ms | Pub/sub |

### Why Redis?

```
OpAMP Core Question: "Which supervisor has device-5?"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Redis      â”‚  GET device:device-5:supervisor
â”‚    0.5 ms âœ…    â”‚  â†’ "supervisor-pod-3"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… Sub-millisecond lookups (critical for 1M+ devices)
âœ… Built-in TTL (auto-cleanup when devices disconnect)
âœ… Simple key-value model (exactly what we need)
âœ… Already available in Aruba (Elasticache)
âœ… Trivial memory footprint (~200MB for 1M devices)
```

### Redis Data Model (Complete)

```redis
# Core routing - THE critical path (every command uses this)
SET device:device-5:supervisor "supervisor-pod-3"
EXPIRE device:device-5:supervisor 300  # Auto-cleanup on disconnect

# Device status (for dashboard)
HSET device:device-5 status "online" emission "true" config_version "v1.2.3"

# Supervisor tracking (for load balancing new connections)
INCR supervisor:supervisor-pod-3:device_count
SADD supervisor:supervisor-pod-3:devices "device-5"

# Config templates (cached, rarely changes)
SET config:fluentbit:v1.2.3 "<config data>"
```

**Total Redis memory for 1M devices: ~100-200 MB** (trivial)

### Why Kafka (Not Direct Calls)

| Without Kafka | With Kafka |
|---------------|------------|
| Server must know all supervisor IPs | Server publishes to topic |
| If supervisor down, command lost | Command persisted, delivered when supervisor recovers |
| Tight coupling | Loose coupling |
| No audit trail | Kafka retention = command history |

### Summary: What OpAMP Uses

| Component | Purpose |
|-----------|--------|
| **Redis/Elasticache** | Deviceâ†’Supervisor routing, status cache, config templates |
| **Kafka** | Command delivery, durability, audit trail |
| **DDS** | Observability platform for cloud services (Server, Supervisor) telemetry |

### DDS Integration (Cloud Services Observability)

| DDS Component | Signal | Source | Use Case |
|---------------|--------|--------|----------|
| **Grafana Loki** | Logs | Server + Supervisor pods | Structured logs, error tracking |
| **Grafana Mimir** | Metrics | Server + Supervisor pods | Request rates, latency, device counts |
| **Grafana Tempo** | Traces | Server + Supervisor pods | Request tracing, end-to-end latency |

> **Note:** Other Aruba databases (CockroachDB, ArangoDB, ClickHouse) are not needed for OpAMP core functionality. They can be added later if analytics or audit log queries are required.

## Existing Infrastructure Leverage

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Used for OpAMP (Already Have)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… Redis/Elasticache (state + routing) â”‚
â”‚  âœ… Kafka (command delivery)            â”‚
â”‚  âœ… DDS (Loki, Mimir, Tempo)            â”‚
â”‚  âœ… Kubernetes                          â”‚
â”‚  âœ… Load balancers                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Available but NOT Needed for OpAMP    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â¸ï¸  CockroachDB (keep for other uses)   â”‚
â”‚  â¸ï¸  ArangoDB (keep for other uses)      â”‚
â”‚  â¸ï¸  ClickHouse (keep for other uses)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          New Components                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ†• OpAMP Server pods (OTel instrumented)|
â”‚  ğŸ†• Supervisor pods (OTel instrumented) â”‚
â”‚  ğŸ†• Device agents (on each device)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## The Simple Truth

```
OpAMP needs to answer ONE question fast:
"Which supervisor has device-X?"

Redis: 0.5ms
Everything else: Slower or wrong tool.

OpAMP needs to deliver commands reliably:
Kafka: Durable, ordered, already available.

That's it. Keep it simple.
```

---

# 5. Scaling Strategy

## Capacity Planning

| Scale | Devices | Supervisors | Servers | Kafka Partitions |
|-------|---------|-------------|---------|------------------|
| Small | 10K | 1 | 2 | 4 |
| Medium | 100K | 5 | 3 | 16 |
| Large | 500K | 25 | 5 | 32 |
| Enterprise | 1M+ | 50+ | 10 | 64 |

## Supervisor Pod Sizing

```
Each Supervisor Pod:
â”œâ”€â”€ Memory: 2-4 GB
â”‚   â”œâ”€â”€ 20K connections Ã— 50KB each = 1GB
â”‚   â””â”€â”€ Overhead, buffers = 1-3GB
â”œâ”€â”€ CPU: 2-4 cores
â”‚   â””â”€â”€ gRPC handling, config processing
â””â”€â”€ Network: 1 Gbps
    â””â”€â”€ Config pushes, heartbeats
```

## Redis Sizing

```
For 1M devices:
â”œâ”€â”€ Keys: ~3M (device routing + status + configs)
â”œâ”€â”€ Average value size: ~50 bytes
â”œâ”€â”€ Total memory: ~150-200 MB
â”œâ”€â”€ Elasticache node: cache.r6g.large (plenty of headroom)
â””â”€â”€ Replication: 1 primary + 1 replica for HA
```

---

# 6. UI at Scale

## The Challenge

The current POC dashboard loads all devices on page load. This won't work at 1M scale:

```
POC Dashboard:
â”œâ”€â”€ Fetches ALL devices on page load
â”œâ”€â”€ Renders each device as DOM element
â”œâ”€â”€ Browser memory: 100 devices = OK, 1M devices = ğŸ’¥ crash
â””â”€â”€ User experience: No one scrolls through 1M rows
```

## Production UI Strategy

At 1M scale, **no one manages devices one-by-one**. The UI shifts to aggregated views + search + bulk operations:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OPO Dashboard (Production)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ğŸ“Š Overview Dashboard (Default View)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 1,000,000   â”‚ â”‚   998,500   â”‚ â”‚    1,500    â”‚ â”‚   12      â”‚ â”‚
â”‚  â”‚ Total       â”‚ â”‚ Online      â”‚ â”‚ Offline     â”‚ â”‚ Errors    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  ğŸ“ˆ Emission Status                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  850K logs ON, 150K logs OFF    â”‚  â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  600K metrics ON, 400K OFF      â”‚  â”‚
â”‚  â”‚ â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  100K traces ON, 900K OFF       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  ğŸ” Search / Filter                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ [device-id contains: _____] [status: â–¼] [type: â–¼]       â”‚  â”‚
â”‚  â”‚ [region: â–¼] [config version: â–¼] [emission: â–¼]           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  ğŸ“‹ Results (paginated, 50 per page)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â˜ device-abc-001  â”‚ Online â”‚ v1.2.3 â”‚ Logs: ON â”‚ Region: A â”‚â”‚
â”‚  â”‚ â˜ device-abc-002  â”‚ Online â”‚ v1.2.3 â”‚ Logs: ON â”‚ Region: A â”‚â”‚
â”‚  â”‚ â˜ device-xyz-100  â”‚ Offlineâ”‚ v1.2.1 â”‚ Logs: OFFâ”‚ Region: B â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  [< Prev] Page 1 of 20,000 [Next >]                            â”‚
â”‚                                                                 â”‚
â”‚  âš¡ Bulk Actions (applied to filtered results)                  â”‚
â”‚  [Toggle Logs ON] [Toggle Logs OFF] [Push Config v1.2.4]       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Design Principles

| Principle | Implementation |
|-----------|----------------|
| **Never load all devices** | Server-side pagination (50 per page) |
| **Aggregated stats** | Redis `SCARD`, pre-computed metrics |
| **Search/Filter** | Server-side query, return page of results |
| **Bulk operations** | "Apply to filter" not "select checkboxes" |
| **Async processing** | Bulk ops queued via Kafka, show progress |

## API Design for Scale

```go
// POC (won't scale)
GET /api/devices â†’ returns ALL devices âŒ

// Production (paginated + filtered)
GET /api/devices?page=1&limit=50&status=online&region=US â†’ returns 50 devices âœ…

// Aggregations (for dashboard widgets)
GET /api/stats â†’ { total: 1000000, online: 998500, offline: 1500, ... }

// Bulk operations (by filter, not by ID list)
POST /api/bulk/toggle
{
  "filter": { "region": "US", "status": "online" },
  "action": "enable_logs"
}
â†’ { "queued": true, "affected": 250000, "job_id": "abc123" }
```

## Technology Choice: HTML + Vanilla JS

| Requirement | Approach | Why It Works |
|-------------|----------|---------------|
| Render device list | HTML table, 50 rows | Trivial for any browser |
| Pagination | Server-side, JS updates table | Simple fetch + innerHTML |
| Filters | Server-side query | No client-side filtering needed |
| Real-time stats | Periodic fetch (5s) | Vanilla JS setInterval |
| Bulk actions | POST to API | No complex state |

**Browser never sees 1M devices.** It only sees:
- 4-5 aggregate numbers
- 50 device rows (one page)
- Filter controls

**Total DOM elements: ~200** (trivial for any browser)

## When to Consider React/Vue

| Scenario | Recommendation |
|----------|----------------|
| **Functional admin dashboard** | HTML + Vanilla JS (sufficient) |
| **Real-time WebSocket updates** | Consider React (state management) |
| **Complex nested filters** | Consider React (component reuse) |
| **Customer-facing polished UI** | React with design system |

**For OPO MVP:** Vanilla JS is sufficient. Backend engineer can build it.

---

# 7. Implementation Roadmap

## Phase 1: POC Enhancement (Current)
- [x] Single server, single supervisor
- [x] 1 device working
- [x] Hot reload with FluentBit API
- [x] Dashboard with toggle controls

## Phase 2: Add Shared State
- [x] Redis/Elasticache *(already available in CNX cluster)*
- [x] Kafka *(already available in CNX cluster)*
- [ ] Migrate device registry to Redis
- [ ] Add Kafka producer to Server
- [ ] Add Kafka consumer to Supervisor
- [ ] Add OTel instrumentation (no stdout, export to DDS)

## Phase 3: Multi-Pod Deployment
- [ ] Scale Server to 3 replicas
- [ ] Scale Supervisor to 5 replicas
- [ ] Test failover scenarios
- [ ] Load test with 1000 devices

## Phase 4: Production Hardening
- [ ] Add authentication/authorization
- [ ] Implement rate limiting
- [ ] Add comprehensive monitoring (DDS dashboards)
- [ ] Runbook and documentation
- [ ] Security audit

## Phase 5: Scale to 1M
- [ ] Gradual rollout to production devices
- [ ] Performance tuning
- [ ] Capacity expansion as needed

---

# Summary

## Architecture Benefits

| Benefit | How Achieved |
|---------|--------------|
| **High Availability** | Multiple Server & Supervisor pods |
| **Horizontal Scale** | Stateless servers, partitioned supervisors |
| **Durability** | Redis for state, Kafka for commands |
| **Low Latency** | gRPC streaming, Redis lookups (0.5ms) |
| **Auditability** | All commands logged to Kafka (retention) |
| **Observability** | Cloud services (Server/Supervisor) instrumented with OTel â†’ DDS |
| **Operational Simplicity** | Uses existing Redis + Kafka + DDS infrastructure |

## Key Metrics to Monitor

| Metric | Target |
|--------|--------|
| Config push latency | < 1 second |
| Hot reload success rate | > 99.9% |
| Device connection uptime | > 99.95% |
| Command delivery latency | < 500ms |

---

# Appendix A: Protocol Details

## OpAMP (Server â†” Supervisor)

```
Protocol: WebSocket
Port: 4320
Library: open-telemetry/opamp-go
Direction: Bidirectional
Messages: AgentToServer, ServerToAgent
```

## Custom gRPC (Supervisor â†” Device)

```protobuf
service ControlService {
  rpc Control(stream Envelope) returns (stream Envelope);
}

message Envelope {
  oneof body {
    EdgeIdentity register = 1;
    Command command = 2;
    Event event = 3;
    ConfigPush config_push = 4;
    ConfigAck config_ack = 5;
  }
}
```

---

# Appendix B: Kafka Topic Configuration

```yaml
# opamp.commands topic
Topic: opamp.commands
Partitions: 64 (one per supervisor or hash-based)
Replication: 3
Retention: 7 days
Key: supervisor_id
Value: JSON command payload

# Message format
{
  "device_id": "device-5",
  "command": "toggle_emission",
  "payload": {
    "state": true,
    "config_version": "v1.2.3"
  },
  "timestamp": "2026-01-27T10:30:00Z",
  "correlation_id": "uuid-here"
}
```

---

# Appendix C: Database Schema (Complete)

```sql
-- Core Tables
CREATE TABLE device_registry (
    device_id         VARCHAR(64) PRIMARY KEY,
    supervisor_id     VARCHAR(64) NOT NULL,
    agent_type        VARCHAR(32) DEFAULT 'fluentbit',
    platform          VARCHAR(32),
    version           VARCHAR(32),
    connected_at      TIMESTAMP DEFAULT NOW(),
    last_seen         TIMESTAMP DEFAULT NOW(),
    config_version    VARCHAR(64),
    effective_config  TEXT,
    emission_state    BOOLEAN DEFAULT false,
    
    INDEX idx_supervisor (supervisor_id),
    INDEX idx_agent_type (agent_type),
    INDEX idx_last_seen (last_seen)
);

CREATE TABLE config_templates (
    template_id       VARCHAR(64) PRIMARY KEY,
    name              VARCHAR(128),
    agent_type        VARCHAR(32),
    version           VARCHAR(32),
    config_data       TEXT NOT NULL,
    is_active         BOOLEAN DEFAULT true,
    created_by        VARCHAR(64),
    created_at        TIMESTAMP DEFAULT NOW(),
    updated_at        TIMESTAMP DEFAULT NOW()
);

CREATE TABLE command_audit (
    id                BIGSERIAL PRIMARY KEY,
    device_id         VARCHAR(64),
    supervisor_id     VARCHAR(64),
    command_type      VARCHAR(32),
    payload           JSONB,
    correlation_id    VARCHAR(64),
    status            VARCHAR(16),
    error_message     TEXT,
    initiated_by      VARCHAR(64),
    created_at        TIMESTAMP DEFAULT NOW(),
    completed_at      TIMESTAMP,
    
    INDEX idx_device_id (device_id),
    INDEX idx_created_at (created_at)
);

CREATE TABLE supervisor_registry (
    supervisor_id     VARCHAR(64) PRIMARY KEY,
    pod_name          VARCHAR(128),
    pod_ip            VARCHAR(45),
    device_count      INTEGER DEFAULT 0,
    capacity          INTEGER DEFAULT 20000,
    last_heartbeat    TIMESTAMP DEFAULT NOW(),
    status            VARCHAR(16) DEFAULT 'active',
    
    INDEX idx_status (status)
);
```
